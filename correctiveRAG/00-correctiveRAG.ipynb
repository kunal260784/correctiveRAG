{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f2f6b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_qdrant import QdrantVectorStore, FastEmbedSparse\n",
    "from qdrant_client import QdrantClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "# from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from IPython.display import display, Image\n",
    "from typing import List, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pprint import pprint\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee63f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessLoader:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = QdrantClient(\n",
    "                            url=os.getenv('QDRANT_API_URL'),\n",
    "                            api_key=os.getenv('QDRANT_API_KEY')\n",
    "                        )\n",
    "        self.dense_embedding = HuggingFaceEmbeddings(model = 'sentence-transformers/all-MiniLM-L12-v2')\n",
    "        self.sparse_embedding = FastEmbedSparse(model_name ='Qdrant/bm25')\n",
    "        \n",
    "        self.qdrantdb = QdrantVectorStore(\n",
    "            client=self.client,\n",
    "            collection_name= os.getenv('COLLECTIONNAME'),\n",
    "            embedding=self.dense_embedding,\n",
    "            sparse_embedding=self.sparse_embedding\n",
    "        )\n",
    "        self.wiki_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results= 5, doc_content_chars_max= 2000))\n",
    "        self.tavily_tool = TavilySearch(\n",
    "            max_results=5,\n",
    "            topic=\"general\",\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def getPBIRetriver(self):\n",
    "\n",
    "        return self.qdrantdb.as_retriever(\n",
    "            search_type = 'mmr',\n",
    "                search_kwarg = {\n",
    "                    \"k\":10\n",
    "                }\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def wikiTool(self):\n",
    "        return self.wiki_tool\n",
    "    \n",
    "    @property\n",
    "    def tavilyTool(self):\n",
    "        return self.tavily_tool\n",
    "    \n",
    "    @property\n",
    "    def getllm(self):\n",
    "        return ChatGroq(model='openai/gpt-oss-120b')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9ace0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class structuredstatus(BaseModel):\n",
    "    relevence : str = Field(description=\"\"\" \n",
    "                                        The field is populated based on the validation outcome: \"Yes\" is assigned \n",
    "                                        if the document is deemed relevant, and \"No\" if it is subsequently discarded.\n",
    "                                        \"\"\",\n",
    "                                        default_factory= str)\n",
    "\n",
    "\n",
    "class DocWithStatus(BaseModel):\n",
    "\n",
    "    doc: Document\n",
    "    relevence : str = Field(description=\"\"\" \n",
    "                                        The field is populated based on the validation outcome: \"Yes\" is assigned \n",
    "                                        if the document is deemed relevant, and \"No\" if it is subsequently discarded.\n",
    "                                        \"\"\",\n",
    "                                        default_factory= str)\n",
    "\n",
    "class CorrectiveRAGMetaData(BaseModel):\n",
    "\n",
    "    query : str\n",
    "    doclist : List[DocWithStatus] = Field(default_factory=list,description= \"\"\"\n",
    "                                                                                Field is consolidating all the Documents received from Retriever. \n",
    "                                                                                create list of DocWithStatus type.\n",
    "                                                                            \"\"\")\n",
    "    threshold : float = Field(default_factory=float, description= \"\"\" \n",
    "                                                                    Determines the currectness of retrived Document. \n",
    "                                                                    It is decimal number, representing Percentage of Currectness.\n",
    "                                                                    \"\"\")\n",
    "    answer: str = Field(default_factory=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a19fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = ProcessLoader()\n",
    "pbiRetriever = obj.getPBIRetriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9875853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docRetreiver(state: CorrectiveRAGMetaData) -> CorrectiveRAGMetaData:\n",
    "\n",
    "    \"\"\" \n",
    "    Retrieves relevant documents from a vector store based on a query sent by LLM. \n",
    "    This node handles the critical step of fetching context based on question attribute of state.\n",
    "\n",
    "    :param state: object of CorrectiveRAGMetaData include below members.\n",
    "                    a. query (String)\n",
    "                    b. doclist (List of Document)\n",
    "                    c. threshold (string)\n",
    "                    d. answer (String)\n",
    "\n",
    "    :type : CorrectiveRAGMetaData\n",
    "\n",
    "    :returs : Object of CorrectiveRAGMetaData type where doclist will be updated by retriver (getPBIRetriver)  \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print('---- In generateRetriverDocs-------')\n",
    "    docs = pbiRetriever.invoke(state.query)\n",
    "    return state.model_copy(update={\n",
    "        \"doclist\" : [DocWithStatus(doc= doc, relevence= \"\") for doc in docs]\n",
    "    })\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6aecf858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateDocs(state: CorrectiveRAGMetaData) -> CorrectiveRAGMetaData:\n",
    "    \"\"\" \n",
    "    The node performs relevance validation on the retrieved document list. \n",
    "    It processes each document and filters out those identified as irrelevant with the help of LLM, thereby ensuring the quality of the final result set.\n",
    "\n",
    "    :param state: object of CorrectiveRAGMetaData include below members.\n",
    "                    a. query (String)\n",
    "                    b. doclist (List of Document)\n",
    "                    c. threshold (string)\n",
    "                    d. answer (String)\n",
    "\n",
    "    :type : CorrectiveRAGMetaData\n",
    "\n",
    "    :returns : Update the Doc List by discarding docs which are irrelevent. Also Calculate Threshold Percentage.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_instruction = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "                If the document is relevant, output only the word 'yes'. If it is not relevant, output only the word 'no'.\n",
    "                Your response MUST be a single word: 'yes' or 'no'. Do not output anything else.\"\"\"\n",
    "\n",
    "    llm_base = obj.getllm\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_instruction),\n",
    "            # The human message takes the dynamic input variables\n",
    "            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    doclist = []\n",
    "    for docobj in state.doclist:\n",
    "\n",
    "        doc_grade_retriever = grade_prompt | llm_base\n",
    "        result = doc_grade_retriever.invoke({\"question\" : state.query,\n",
    "                                    \"document\" : docobj.doc.page_content})\n",
    "        \n",
    "        print(result)\n",
    "        if result.content.strip().lower() == 'yes':\n",
    "            update_docobj =  DocWithStatus(doc= docobj.doc, relevence= result.content.strip().lower())\n",
    "            doclist.append(update_docobj)\n",
    "\n",
    "    threshold = len(doclist) / len(state.doclist) if len(state.doclist) != 0 else -1\n",
    "\n",
    "    return state.model_copy(update={\n",
    "        \"doclist\" : doclist,\n",
    "        \"threshold\" : threshold\n",
    "    })\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e8bf61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decideThreshold(state: CorrectiveRAGMetaData):\n",
    "\n",
    "    \"\"\" \n",
    "    This will work as consitional Edge and deciding if Web Search is required or Not. \n",
    "    If State Threshold Value is less then 70% then it will return Web Seach Function.\n",
    "    If State Threshold Value is greater then Or Equal to 70% then it will generate Output.\n",
    "\n",
    "    Also it Considers No of Document Retured. \n",
    "    If retriever returns 3 or more Valid document then it will generate Answer.\n",
    "    If retriever returns less then 3 Valid document then it will go for Web Search.\n",
    "\n",
    "    :param state: object of CorrectiveRAGMetaData include below members.\n",
    "                    a. query (String)\n",
    "                    b. doclist (List of Document)\n",
    "                    c. threshold (string)\n",
    "                    d. answer (String)\n",
    "\n",
    "    :type : CorrectiveRAGMetaData\n",
    "\n",
    "    :returns : Function Name need to call in next step.\n",
    "    \"\"\"\n",
    "    print(f\"no of docs {len(state.doclist)} and Threshold is {state.threshold}\")\n",
    "    if len(state.doclist) >= 3 and state.threshold >= 0.7:\n",
    "        print(\"generate\")\n",
    "        return \"generateAnswer\"\n",
    "    else:\n",
    "        print(\"Web Search\")\n",
    "        return \"webSearch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31d1f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAnswer(state: CorrectiveRAGMetaData) -> CorrectiveRAGMetaData:\n",
    "\n",
    "    \"\"\"\n",
    "        Synthesizes the final answer by consolidating retrieved documents.\n",
    "\n",
    "        This terminal node in the LLM chain takes the list of relevant documents, \n",
    "        feeds them into the Language Model (LLM) as context, and prompts the LLM to generate a single, coherent, and\n",
    "        comprehensive answer that addresses all parts of the original query.\n",
    "\n",
    "        :param state: object of CorrectiveRAGMetaData include below members.\n",
    "                    a. query (String)\n",
    "                    b. doclist (List of Document)\n",
    "                    c. threshold (string)\n",
    "                    d. answer (String)\n",
    "\n",
    "        :type : CorrectiveRAGMetaData\n",
    "\n",
    "        :returs : Object of CorrectiveRAGMetaData type where answer will be populated by LLM based on Documents received from retriver (pbi_retriver)  \n",
    "    \"\"\"\n",
    "    print('---- In generateAnswer-------')\n",
    "    context_documents = \"\\n\\n\".join([objdoc.doc.page_content for objdoc in state.doclist])\n",
    "    prompt = f\"\"\" \n",
    "             You are an expert Q&A system. Your task is to generate a single, comprehensive, and well-structured answer to the user's original query.\n",
    "\n",
    "                    **Instructions:**\n",
    "                    1.  Read the **Original Query** \n",
    "                    2.  Carefully analyze the content of the **Context Documents** provided below.\n",
    "                    3.  Synthesize the information from the documents to construct a complete answer that addresses all parts of the Original Query.\n",
    "                    4.  Do not introduce any information that is not explicitly present in the Context Documents.\n",
    "                    5.  Structure your final answer clearly using markdown headings and bullet points where appropriate.\n",
    "                    6.  Refer Docstring for more details\n",
    "\n",
    "                    **Original Query:**\n",
    "                    {state.query}\n",
    "\n",
    "                    **Context Documents:**\n",
    "                    {context_documents}\n",
    "\n",
    "                    **Final Answer:**\n",
    "\n",
    "            \"\"\"\n",
    "    \n",
    "    result = obj.getllm.invoke(prompt).content\n",
    "    return state.model_copy(\n",
    "        update={\n",
    "            \"answer\": result\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8343ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrompt() -> str:\n",
    "\n",
    "    return f\"\"\" \n",
    "                ### üìù GENERIC RESEARCH MANDATE ###\n",
    "\n",
    "                    **PRIMARY TOPIC:** Based on User Query Topic should be taken\n",
    "\n",
    "                    **RESEARCH OBJECTIVE:**\n",
    "                    You are an expert, objective research analyst. Your task is to produce a comprehensive, multi-faceted report on the **PRIMARY TOPIC** that achieves a deep understanding of its current state, challenges, and future trajectory.\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ### üîç RESEARCH SCOPE & CONSTRAINTS ###\n",
    "                    * **Timeframe:** Focus primarily on developments and data from the last **5 years** (e.g., 2020‚Äìpresent), unless historical context is strictly necessary to understand the current state.\n",
    "                    * **Depth:** Research must move beyond Wikipedia-level summaries and include synthesis, analysis, and critical evaluation of information.\n",
    "                    * **Exclusions:** Avoid pure dictionary definitions or tangential subjects. Focus exclusively on the topic's direct implications.\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ### üîë MANDATORY REPORT SECTIONS ###\n",
    "\n",
    "                    The final output MUST be organized into the following four distinct sections.\n",
    "\n",
    "                    **SECTION 1: Overview & Current State**\n",
    "                    * **Goal:** Define the topic and establish its importance.\n",
    "                    * **Content:** Provide a concise, professional definition. Detail the topic's current scale, major players, or widely accepted status quo.\n",
    "\n",
    "                    **SECTION 2: Key Challenges & Controversies**\n",
    "                    * **Goal:** Identify and analyze critical obstacles.\n",
    "                    * **Content:** Identify and analyze the **three most significant challenges** or **major points of controversy** surrounding the topic. For each challenge, explain the nature of the problem and its primary contributing factors.\n",
    "\n",
    "                    **SECTION 3: Emerging Trends & Future Trajectory**\n",
    "                    * **Goal:** Forecast and highlight new developments.\n",
    "                    * **Content:** Identify and analyze **two major emerging trends** or **potential future applications/solutions** that are likely to shape the topic over the next 5‚Äì10 years.\n",
    "\n",
    "                    **SECTION 4: Conclusion & Strategic Implications**\n",
    "                    * **Goal:** Summarize findings and provide actionable takeaways.\n",
    "                    * **Content:** Summarize the main analytical findings. Answer this critical question: **\"What is the single most important strategic implication for an organization or individual engaging with this topic?\"**\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ### ‚ú® OUTPUT FORMATTING RULES ###\n",
    "                    1.  **Tone:** Maintain an **analytical, objective, and professional** tone throughout.\n",
    "                    2.  **Citations (Fictional):** For each of the three mandatory challenges and two emerging trends, invent and list a realistic-sounding **source type** (e.g., \"[Source: 2024 Market Analysis Report]\", \"[Source: Nature Communications Paper, 2023]\"). This simulates a research process and improves analytical depth.\n",
    "                    3.  **Formatting:** Use markdown headings and bolding to clearly delineate the four sections and their sub-points.\n",
    "\n",
    "                   \n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5b302ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def webSearch(state: CorrectiveRAGMetaData) -> CorrectiveRAGMetaData:\n",
    "\n",
    "    \"\"\" \n",
    "    Based on Threshold, If Threshold Value is Less then 70%, Question is taken by React Agent. \n",
    "    It Consolidates all the Valid Previous Output with Web Search and regenerate DocList.\n",
    "\n",
    "    :param state: object of CorrectiveRAGMetaData include below members.\n",
    "                    a. query (String)\n",
    "                    b. doclist (List of Document)\n",
    "                    c. threshold (string)\n",
    "                    d. answer (String)\n",
    "\n",
    "    :type : CorrectiveRAGMetaData\n",
    "\n",
    "    :returs : Object of CorrectiveRAGMetaData type where new List is upended to original DocList\n",
    "    \"\"\"\n",
    "\n",
    "    tools = [obj.wikiTool, obj.tavilyTool]\n",
    "\n",
    "    web_agent=create_agent(\n",
    "                obj.getllm,\n",
    "                tools=tools,\n",
    "                system_prompt=getPrompt()\n",
    "    )\n",
    "\n",
    "    web_result = web_agent.invoke({\"messages\" :state.query})\n",
    "    # print(web_result['messages'][-1].content)\n",
    "\n",
    "    doc = DocWithStatus(doc = Document(page_content=web_result['messages'][-1].content), relevence='yes')\n",
    "    state.doclist.append(doc)\n",
    "\n",
    "    return state.model_copy(update={\n",
    "        \"doclist\" : state.doclist\n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92f3967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = CorrectiveRAGMetaData(\n",
    "    query=\"What is Power BI?\"   ,\n",
    "    documents=[], # Provide an empty list\n",
    "    messages=[], # Provide an empty list\n",
    "    answer=\"\" # Provide an empty string (or whatever type 'answer' is) \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c2e5861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- In generateRetriverDocs-------\n",
      "content='yes' additional_kwargs={'reasoning_content': 'We need to decide if the retrieved document is relevant to the user question \"What is Power BI?\" The document defines what Power BI is, includes description. So yes.'} response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 428, 'total_tokens': 473, 'completion_time': 0.100100264, 'completion_tokens_details': {'reasoning_tokens': 35}, 'prompt_time': 0.020124351, 'prompt_tokens_details': None, 'queue_time': 0.034495723, 'total_time': 0.120224615}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4867cb64c5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--912efd13-18df-4e5d-93d9-be746b9f95e7-0' usage_metadata={'input_tokens': 428, 'output_tokens': 45, 'total_tokens': 473, 'output_token_details': {'reasoning': 35}}\n",
      "content='no' additional_kwargs={'reasoning_content': 'We need to decide if the retrieved document is relevant to the user question \"What is Power BI?\" The retrieved doc snippet: \"specific Power BI transformation or a deeper dive into DAX?\" That\\'s about Power BI transformations or DAX. The user asks \"What is Power BI?\" The doc seems not directly answering what Power BI is; it\\'s about specific transformation or DAX, not a definition. So not relevant. Output \"no\".'} response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 163, 'total_tokens': 261, 'completion_time': 0.216481006, 'completion_tokens_details': {'reasoning_tokens': 88}, 'prompt_time': 0.00651055, 'prompt_tokens_details': None, 'queue_time': 0.034293424, 'total_time': 0.222991556}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_07c3a8857b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--36b6cf02-a484-478c-b813-f03f422a86c1-0' usage_metadata={'input_tokens': 163, 'output_tokens': 98, 'total_tokens': 261, 'output_token_details': {'reasoning': 88}}\n",
      "content='yes' additional_kwargs={'reasoning_content': 'We need to decide if the retrieved document is relevant to the user question \"What is Power BI?\" The retrieved document says \"That\\'s a comprehensive set of questions about Power BI fundamentals! Here are the answers: I. Fundamentals of Power BI 1.\" This seems to be about Power BI fundamentals, likely includes an answer to \"What is Power BI?\" It is relevant. So output \"yes\".'} response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 176, 'total_tokens': 266, 'completion_time': 0.192865862, 'completion_tokens_details': {'reasoning_tokens': 80}, 'prompt_time': 0.007076804, 'prompt_tokens_details': None, 'queue_time': 0.004599734, 'total_time': 0.199942666}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_c5da83ab22', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--f3d4b2b0-b454-48da-9406-fd72274c5633-0' usage_metadata={'input_tokens': 176, 'output_tokens': 90, 'total_tokens': 266, 'output_token_details': {'reasoning': 80}}\n",
      "content='yes' additional_kwargs={'reasoning_content': 'We need to decide if the retrieved document is relevant to the question \"What is Power BI?\" The document snippet mentions Power BI renders the resulting plot/image output of the script as a static visual on the report canvas. It also mentions visualization, script uses libraries like matplotlib etc. This is somewhat about Power BI\\'s functionality. The question asks \"What is Power BI?\" The document provides a small piece describing that Power BI renders visual output. That is somewhat relevant: it mentions Power BI and its role. It is partially relevant, but does it answer \"What is Power BI?\" It gives a description that Power BI renders static visuals. That\\'s a description of a feature, so it\\'s relevant. According to instructions, if relevant, output \"yes\". So answer yes.'} response_metadata={'token_usage': {'completion_tokens': 163, 'prompt_tokens': 206, 'total_tokens': 369, 'completion_time': 0.376722719, 'completion_tokens_details': {'reasoning_tokens': 153}, 'prompt_time': 0.008364074, 'prompt_tokens_details': None, 'queue_time': 0.00458773, 'total_time': 0.385086793}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_c5da83ab22', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--7b9a66b2-de7c-4daf-98d5-2d91e4c34d95-0' usage_metadata={'input_tokens': 206, 'output_tokens': 163, 'total_tokens': 369, 'output_token_details': {'reasoning': 153}}\n",
      "---- In generateAnswer-------\n"
     ]
    }
   ],
   "source": [
    "st_obj = docRetreiver(state=state)\n",
    "st_obj = validateDocs(state=st_obj)\n",
    "st_obj = generateAnswer(state=st_obj)\n",
    "web_agent = webSearch(state=st_obj)\n",
    "\n",
    "# result = st_obj.messages\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23a6085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(CorrectiveRAGMetaData)\n",
    "\n",
    "graph.add_node(\"docRetreiver\", docRetreiver)\n",
    "graph.add_node(\"validateDocs\", validateDocs)\n",
    "graph.add_node(\"generateAnswer\", generateAnswer)\n",
    "graph.add_node(\"webSearch\", webSearch)\n",
    "\n",
    "# 1. Start the workflow\n",
    "graph.add_edge(START, \"docRetreiver\")\n",
    "\n",
    "# 2. Sequential step\n",
    "graph.add_edge(\"docRetreiver\", \"validateDocs\")\n",
    "\n",
    "# 3. Conditional branch based on validation\n",
    "graph.add_conditional_edges(\"validateDocs\",\n",
    "                             decideThreshold,\n",
    "                             {\n",
    "                                 \"generateAnswer\": \"generateAnswer\",  # Valid docs -> Generate Answer\n",
    "                                 \"webSearch\": \"webSearch\"     # Invalid docs -> Web Search\n",
    "                             }\n",
    "                           )\n",
    "\n",
    "# 4. Rejoin the main path after Web Search\n",
    "graph.add_edge(\"webSearch\", \"generateAnswer\")\n",
    "\n",
    "# 5. Connect the final step to the END state (This was the missing/commented part)\n",
    "graph.add_edge(\"generateAnswer\", END) \n",
    "\n",
    "# Compile the graph\n",
    "graph_builder = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d4376c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- In generateRetriverDocs-------\n",
      "content='no' additional_kwargs={'reasoning_content': 'We need to decide if the retrieved document is relevant to the user question: \"What Power BI Q&A feature?\" The document says: \"That\\'s an extensive list covering Power BI performance, advanced features, and scenario-based questions! Here are the answers.\" That\\'s generic, not specifically about Q&A feature. It doesn\\'t mention Q&A. So not relevant. Output \"no\".'} response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 175, 'total_tokens': 260, 'completion_time': 0.205048326, 'completion_tokens_details': {'reasoning_tokens': 75}, 'prompt_time': 0.007742901, 'prompt_tokens_details': None, 'queue_time': 0.03439204, 'total_time': 0.212791227}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4867cb64c5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--736ac3fa-ad51-40a9-b638-e8d4ffda22d5-0' usage_metadata={'input_tokens': 175, 'output_tokens': 85, 'total_tokens': 260, 'output_token_details': {'reasoning': 75}}\n",
      "content='no' additional_kwargs={'reasoning_content': 'We need to decide if the retrieved document is relevant to the user question. The user asks: \"What Power BI Q&A feature?\" They want info about Power BI Q&A feature. The retrieved doc: \"M & DAX: I use comments within the M code and DAX measures to explain complex logic. 147. Describe a time when you had to learn a new Power BI feature or concept quickly. How did you approach it?\" This mentions Power BI feature but not Q&A specifically. It\\'s about using comments, describing learning new feature, but not about Q&A. So not relevant. Output \"no\".'} response_metadata={'token_usage': {'completion_tokens': 135, 'prompt_tokens': 202, 'total_tokens': 337, 'completion_time': 0.295896492, 'completion_tokens_details': {'reasoning_tokens': 125}, 'prompt_time': 0.013234118, 'prompt_tokens_details': None, 'queue_time': 0.035618446, 'total_time': 0.30913061}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4867cb64c5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--e00bfbb5-1a5e-4eef-b9bd-90b7e8e5223c-0' usage_metadata={'input_tokens': 202, 'output_tokens': 135, 'total_tokens': 337, 'output_token_details': {'reasoning': 125}}\n",
      "content='no' additional_kwargs={'reasoning_content': 'We need to decide if the retrieved document is relevant to the user question. Document: \"90. How can you ensure your Power BI reports are accessible to users with disabilities? Accessibility ensures users with visual, auditory, cognitive, or physical impairments can effectively use and understand your reports.45\". User question: \"What Power BI Q&A feature?\" The document is about accessibility, not Q&A. So not relevant. Answer: no.'} response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 194, 'total_tokens': 293, 'completion_time': 0.214912649, 'completion_tokens_details': {'reasoning_tokens': 89}, 'prompt_time': 0.007724848, 'prompt_tokens_details': None, 'queue_time': 0.072674164, 'total_time': 0.222637497}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_eec389997b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--4be6e4f1-177f-48c5-a0b8-c17e05154863-0' usage_metadata={'input_tokens': 194, 'output_tokens': 99, 'total_tokens': 293, 'output_token_details': {'reasoning': 89}}\n",
      "content='no' additional_kwargs={'reasoning_content': 'We need to determine if the retrieved document is relevant to the user question: \"What Power BI Q&A feature?\" The document describes what Power BI is, its purpose, capabilities, and main components (Desktop, Service, Mobile). It does not mention Q&A feature. So it\\'s not relevant. Answer \"no\".'} response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 430, 'total_tokens': 504, 'completion_time': 0.156955813, 'completion_tokens_details': {'reasoning_tokens': 64}, 'prompt_time': 0.015939171, 'prompt_tokens_details': {'cached_tokens': 256}, 'queue_time': 0.004130578, 'total_time': 0.172894984}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_c5da83ab22', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--847157d2-5282-4644-8657-92e41156ac1d-0' usage_metadata={'input_tokens': 430, 'output_tokens': 74, 'total_tokens': 504, 'input_token_details': {'cache_read': 256}, 'output_token_details': {'reasoning': 64}}\n",
      "no of docs 0 and Threshold is 0.0\n",
      "Web Search\n",
      "---- In generateAnswer-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What Power BI Q&A feature?',\n",
       " 'doclist': [DocWithStatus(doc=Document(metadata={}, page_content='## **SECTION 1 ‚Äì Overview & Current State**\\n\\n**What is Power\\u202fBI Q&A?**  \\nPower\\u202fBI\\u202fQ&A is the natural‚Äëlanguage query engine embedded in Microsoft\\u202fPower\\u202fBI that lets users type questions (e.g., ‚Äútotal sales by region last quarter‚Äù) and instantly receive visual answers‚Äîcharts, tables, or maps‚Äîgenerated on the fly from the underlying data model. It leverages a semantic layer (the *model‚Äôs* tables, columns, relationships, and metadata) plus built‚Äëin natural‚Äëlanguage processing (NLP) to translate user intent into DAX (Data Analysis Expressions) queries.\\n\\n**Why it matters today**  \\n- **Self‚Äëservice analytics:** Q&A removes the need for every stakeholder to learn DAX or build visuals, dramatically shortening the ‚Äúdata‚Äëto‚Äëinsight‚Äù cycle.  \\n- **Adoption driver:** Since its GA in 2016, Microsoft reports that Q&A is now enabled in **‚âà\\u202f70\\u202f%** of new Power\\u202fBI workspaces (Microsoft Power\\u202fBI Adoption Survey\\u202f2023).  \\n- **Ecosystem integration:** Q&A works across Power\\u202fBI Desktop, the Power\\u202fBI Service, and embedded analytics, and it is tightly coupled with the broader Microsoft AI stack (Azure Cognitive Services, Power\\u202fBI Copilot).  \\n\\nThe feature is built on a **semantic model** that must be curated (synonyms, phrasings, and ‚Äúquestion‚Äëanswer‚Äù pairs) to achieve high accuracy. As of 2024, the most recent upgrade introduced **auto‚Äësuggested synonyms** powered by Azure OpenAI, improving the ‚Äúunderstanding‚Äù of colloquial phrasing without manual effort.\\n\\n---\\n\\n## **SECTION 2 ‚Äì Key Challenges & Controversies**\\n\\n| # | Challenge | Core Issue & Contributing Factors |\\n|---|-----------|-----------------------------------|\\n| 1Ô∏è‚É£ | **Natural‚ÄëLanguage Accuracy & Ambiguity** | Q&A still misinterprets vague or domain‚Äëspecific phrasing (e.g., ‚Äúprofit‚Äù vs. ‚Äúgross profit‚Äù). The underlying NLP model is trained on generic corpora, so industry‚Äëspecific jargon often requires extensive synonym mapping. <br> *Contributors:* limited domain‚Äëspecific training data, reliance on exact column names, and ambiguous temporal references. <br> *Impact:* Users receive incorrect visuals, eroding trust. <br> **[Source: 2024 Gartner ‚ÄúBI Platform User Experience‚Äù Report]** |\\n| 2Ô∏è‚É£ | **Data‚ÄëModel Preparation Overhead** | For Q&A to work, the data model must be **semantic‚Äëready**: proper naming conventions, well‚Äëdefined relationships, and curated synonym sets. Many organizations treat Q&A as a ‚Äúfeature toggle‚Äù rather than a **modeling discipline**, leading to high failure rates (‚âà\\u202f45\\u202f% of Q&A attempts return ‚ÄúI don‚Äôt know‚Äù). <br> *Contributors:* legacy data warehouses, lack of data‚Äëmodel governance, and insufficient training for modelers. <br> **[Source: Microsoft Power\\u202fBI Community Survey, 2023]** |\\n| 3Ô∏è‚É£ | **Governance, Security & Bias Concerns** | Q&A can surface any column the user has permission to see, which may unintentionally expose sensitive attributes (e.g., employee IDs). Moreover, the AI‚Äëdriven synonym suggestions can embed hidden biases (e.g., favoring ‚Äúsales‚Äù over ‚Äúrevenue‚Äù in certain regions). <br> *Contributors:* coarse‚Äëgrained row‚Äëlevel security (RLS) implementation, lack of audit logs for Q&A queries, and opaque AI recommendation pipelines. <br> **[Source: 2024 IEEE Security & Privacy Paper on AI‚ÄëAssisted BI]** |\\n\\n---\\n\\n## **SECTION 3 ‚Äì Emerging Trends & Future Trajectory**\\n\\n| # | Emerging Trend | Potential Impact (5‚Äë10\\u202fyr horizon) |\\n|---|----------------|-----------------------------------|\\n| 1Ô∏è‚É£ | **Deep Integration with Large‚ÄëLanguage Models (LLMs)** ‚Äì Power\\u202fBI Copilot & Azure OpenAI | Microsoft is rolling out **Copilot‚Äëdriven Q&A**, where a conversational LLM interprets multi‚Äëturn dialogs, remembers context, and can generate composite visual stories (e.g., ‚ÄúShow me sales trends, then compare them with marketing spend‚Äù). This will shift Q&A from single‚Äëshot queries to **dialog‚Äëbased analytics**, reducing the need for exhaustive synonym libraries. <br> **[Source: Microsoft Build 2024 ‚ÄúPower\\u202fBI Copilot‚Äù Announcement]** |\\n| 2Ô∏è‚É£ | **Real‚ÄëTime & Streaming Q&A** on DirectQuery & Azure Synapse** | Upcoming updates (preview in 2024) enable Q&A over **real‚Äëtime streaming datasets** (e.g., IoT telemetry, live sales feeds). Users will ask ‚ÄúWhat is the current error rate per factory?‚Äù and receive instantly refreshed visuals without pre‚Äëaggregated tables. This expands Q&A from historical analysis to **operational monitoring**, opening new use‚Äëcases in manufacturing, logistics, and finance. <br> **[Source: 2024 Forrester ‚ÄúReal‚ÄëTime BI Market Outlook‚Äù]** |\\n\\n---\\n\\n## **SECTION 4 ‚Äì Conclusion & Strategic Implications**\\n\\n**Bottom‚Äëline synthesis**  \\nPower\\u202fBI Q&A has matured from a novelty feature into a core self‚Äëservice analytics engine, yet its effectiveness hinges on **semantic model quality**, **robust governance**, and **domain‚Äëaware NLP**. The biggest hurdles‚Äîaccuracy, model preparation, and security‚Äîare being addressed through LLM integration and real‚Äëtime capabilities, promising a shift toward conversational, always‚Äëup‚Äëto‚Äëdate insights.\\n\\n**Strategic implication**  \\n> **For any organization adopting Power\\u202fBI Q&A, the single most important strategic move is to institutionalize a *semantic‚Äëmodel governance framework* that couples data‚Äëmodel best practices (naming, relationships, synonym curation) with AI‚Äëassisted validation and security policies.**  \\n\\nOnly by embedding this governance into the data‚Äëengineering lifecycle can firms fully reap the efficiency gains of conversational analytics while safeguarding data integrity and user trust.'), relevence='yes')],\n",
       " 'threshold': 0.0,\n",
       " 'answer': '## Power\\u202fBI Q&A ‚Äì Overview of the Feature  \\n\\n### What it is  \\n- **Power\\u202fBI Q&A** is the **natural‚Äëlanguage query engine** built into Microsoft\\u202fPower\\u202fBI.  \\n- It lets users type plain‚ÄëEnglish questions (e.g., ‚Äútotal sales by region last quarter‚Äù) and instantly receives **visual answers**‚Äîcharts, tables, or maps‚Äîgenerated on the fly from the underlying data model.  \\n\\n### How it works  \\n- **Semantic layer:** Q&A reads the model‚Äôs tables, columns, relationships, and metadata.  \\n- **NLP translation:** Built‚Äëin natural‚Äëlanguage processing converts the user‚Äôs intent into a **DAX** query that runs against the data.  \\n- **Result rendering:** The engine creates an appropriate visual (column chart, map, etc.) and displays it directly in Power\\u202fBI Desktop, the Power\\u202fBI Service, or embedded analytics.  \\n\\n### Why it matters today  \\n- **Self‚Äëservice analytics:** Users can obtain insights without learning DAX or manually building visuals, dramatically shortening the data‚Äëto‚Äëinsight cycle.  \\n- **Broad adoption:** Since its GA in 2016, Q&A is enabled in roughly **70\\u202f% of new Power\\u202fBI workspaces** (Power\\u202fBI Adoption Survey\\u202f2023).  \\n- **Ecosystem integration:** Works across Desktop, Service, and embedded scenarios and is tied to Microsoft‚Äôs wider AI stack (Azure Cognitive Services, Power\\u202fBI\\u202fCopilot).  \\n\\n---\\n\\n## Key Characteristics & Recent Enhancements  \\n\\n| Feature | Description (from the documents) |\\n|---------|-----------------------------------|\\n| **Semantic‚Äëready model requirement** | The model must be curated with proper naming, relationships, and synonym sets for Q&A to answer accurately. |\\n| **Auto‚Äësuggested synonyms (2024)** | Azure OpenAI‚Äëpowered suggestions help the engine understand colloquial phrasing without manual synonym entry. |\\n| **Integration with Copilot & LLMs** | Power\\u202fBI\\u202fCopilot introduces multi‚Äëturn, conversational Q&A that can remember context and build composite visual stories. |\\n| **Real‚Äëtime & streaming Q&A (preview 2024)** | Enables Q&A over DirectQuery and Azure Synapse streaming datasets, allowing instant answers on live data (e.g., IoT telemetry). |\\n\\n---\\n\\n## Benefits  \\n\\n- **Speed:** Instant visual answers replace the time‚Äëconsuming ‚Äúbuild‚Äëa‚Äëvisual‚Äù workflow.  \\n- **Accessibility:** Business users without technical expertise can explore data directly.  \\n- **Consistency:** The same semantic model drives both traditional reports and Q&A, ensuring a single source of truth.  \\n\\n---\\n\\n## Challenges to Keep in Mind  \\n\\n| Challenge | Core Issue (as documented) |\\n|-----------|----------------------------|\\n| **Natural‚Äëlanguage accuracy & ambiguity** | Generic NLP model may misinterpret domain‚Äëspecific terms (e.g., ‚Äúprofit‚Äù vs. ‚Äúgross profit‚Äù). |\\n| **Data‚Äëmodel preparation overhead** | Without a well‚Äëcurated semantic model, ~45\\u202f% of Q&A attempts return ‚ÄúI don‚Äôt know.‚Äù |\\n| **Governance, security & bias** | Q&A can expose any column a user can see, risking sensitive data leakage; AI‚Äëdriven synonym suggestions may embed hidden biases. |\\n\\n*Addressing these challenges typically involves: establishing a semantic‚Äëmodel governance framework, curating synonym libraries, and applying fine‚Äëgrained row‚Äëlevel security.*\\n\\n---\\n\\n## Strategic Takeaway  \\n\\n> **To unlock the full potential of Power\\u202fBI Q&A, organizations should institutionalize a semantic‚Äëmodel governance process that couples disciplined data‚Äëmodel design (naming, relationships, synonym curation) with AI‚Äëassisted validation and robust security controls.**  \\n\\nWhen the underlying model is ‚ÄúQ&A‚Äëready,‚Äù the feature delivers rapid, self‚Äëservice insights while maintaining data integrity and user trust.'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.invoke(CorrectiveRAGMetaData(query=\"What Power BI Q&A feature?\"   ,\n",
    "    documents=[], # Provide an empty list\n",
    "    messages=[], # Provide an empty list\n",
    "    answer=\"\" # Provide an empty string (or whatever type 'answer' is) \n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CorrectiveRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
